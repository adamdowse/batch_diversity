built model with input shape = (28, 28, 1, 1)
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #
=================================================================
 conv2d (Conv2D)             (None, 26, 26, 32)        320
 max_pooling2d (MaxPooling2D  (None, 13, 13, 32)       0
 )
 flatten (Flatten)           (None, 5408)              0
 dense (Dense)               (None, 100)               540900
 dense_1 (Dense)             (None, 10)                1010
=================================================================
Total params: 542,230
Trainable params: 542,230
Non-trainable params: 0
_________________________________________________________________
There are  188  batches in the warm start
Started Training
2022-08-24 15:47:30.443729: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.
Traceback (most recent call last):
  File "Main.py", line 83, in <module>
    sf.k_diversity(model,train_ds,config['k'],config['batch_size'],config['alpha'],config['beta'],conn)
  File "/com.docker.devenvironments.code/supporting_functions.py", line 268, in k_diversity
    grads_to_add.append(g.numpy()-label.numpy())
AttributeError: 'numpy.ndarray' object has no attribute 'numpy'
(1, 28, 28, 1)
tf.Tensor([4], shape=(1,), dtype=int64)
1/1 [==============================] - 0s 87ms/step
[[0.2792162  0.21465498 0.         0.14977671 0.         0.13329229
  0.         0.05002967 0.         0.15217547 0.16355751 0.1503553
  0.         0.18642472 0.         0.         0.         0.
  0.08192997 0.         0.         0.         0.0224857  0.
  0.         0.         0.         0.10985489 0.         0.15047128
  0.         0.22164917 0.00990597 0.         0.0654394  0.04504779
  0.02812552 0.07097258 0.15013179 0.0416446  0.15409723 0.
  0.         0.15092371 0.08021466 0.01109954 0.         0.02455997
  0.         0.02782998 0.32208017 0.         0.21894817 0.06297798
  0.         0.11934316 0.2038003  0.         0.26573598 0.
  0.         0.18427138 0.07960299 0.         0.03938725 0.02160341
  0.         0.         0.         0.         0.         0.08338389
  0.         0.04211903 0.02782514 0.         0.         0.
  0.19139272 0.         0.02202354 0.         0.13560355 0.01150299
  0.16140483 0.         0.         0.         0.         0.12867293
  0.13559406 0.07777707 0.10959709 0.0355599  0.10139377 0.18117307
  0.         0.         0.12142722 0.        ]]